<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<title>Xing Xu </title>
</head>
<body>
<table summary="Table for page layout." id="tlayout">
<tr valign="top">
<td id="layout-menu">
<div class="menu-item"><a href="index.html" class="current">Home</a></div>
<div class="menu-item"><a href="publication.html">Publications</a></div>
</td>
<td id="layout-content">
<div id="toptitle">
<h1>Xing Xu </h1>
</div>
<table class="imgtable"><tr><td>
<img src="pictures/profile_xing.jpg" alt="profile picture" width="150px" height="180px" />&nbsp;</td>
<td align="left"><p><b>Xing Xu (å¾ è¡Œ)</b> <a href="https://scholar.google.com/citations?hl=en&amp;user=aTWbAiEAAAAJ&amp;view_op=list_works&amp;gmla=AJsN-F465dfbHP9CdGL1AfqODfNuclkloKqbIwLVHn2tddZEnnxf4b5TXx4L919LqAjkNxojFs3f6osR9osAMCxTeeyHqIrZnk2w8qYhgBygchDF8Y0PzNgS1N6Ma4ozMU30TbNhRkmT">[Google Scholar]</a>
<br />
Researcher (ç ”ç©¶å‘˜) <br />
<a href="http://cfm.uestc.edu.cn/index">Center for Future Media</a><br />
<a href="http://www.scse.uestc.edu.cn/">School of Computer Science and Engineering</a><br />
<a href="https://www.uestc.edu.cn/">Univeristy of Electronic Science and Technology of China, China</a><br />
Email: xing.xu [at] uestc [dot] edu [dot] cn</p>
</td></tr></table>
<h2>About Me</h2>
<p>I am currently a Researcher with Center of Future Media &amp; School of Computer Science and Engineering, University of Electronic of Science and Technology of China (UESTC), China. I received the B.E. and M.E. degrees from Huazhong University of Science and Technology, China, in 2009 and 2012, respectively, and the Ph.D. degree from Kyushu University, Japan, in 2015. Before joining UESTC, I worked as a Research Technician at Kyushu University (2015-2016). <br /></p>
<p>My research interests mainly focus on multimedia information retrieval, especially cross-modal retrieval and knowledge transfer in integrating language and vision. I have published 100+ academic papers (50+ in CVPR, ICCV, ACM Multimedia, ACM SIGIR, AAAI, TPAMI, TIP, TMM, TCSVT, TCYB, TNNLS). 
I am the recipient of the Best Paper Award at ACM MM 2017, the World's FIRST 10K Best Paper Award - Platinum Award at IEEE ICME 2017, the Best Paper Award at ISAIR 2020 and IEEE Multimedia Prize Paper Award 2020.
In 2019, I received the Outstanding Young Researcher Award from the IEEE Computer Society, Special Technical Committee on Big Data.</p>
<p><FONT COLOR="FF0000">æ¬¢è¿æœ‰å¿—äºä»äº‹è®¡ç®—æœºè§†è§‰ã€äººå·¥æ™ºèƒ½æ–¹å‘å­¦æœ¯ç ”ç©¶å’Œç®—æ³•å¼€å‘çš„åŒå­¦æŠ¥è€ƒç ”ç©¶ç”Ÿ~ åŒæ—¶ä¹Ÿæ¬¢è¿è®¡åˆ’ä¿ç ”å’Œå‡ºå›½çš„åŒå­¦æå‰å‚ä¸å®éªŒå®¤ç§‘ç ”è®­ç»ƒ~</FONT> <br /></p>
<h2>News</h2>
<ul>
<li><p><i>2022/07/02</i> &ndash; 4 papers about Adversarial Attack on Image Retrieval, Visual-Audio Event Localization, Zero-shot Sketch-based Image Retrieval, Visual-Audio Fine-Grained Recognition are accepted by ACM Multimedia 2022! Congratulations to Siyuan, Xun, Kai and Xiaoyu! <FONT COLOR="FF0000">[NEW]</FONT> <br /></p>
</li>
</ul>
<ul>
<li><p><i>2022/04/08</i> &ndash; 2 papers about Black-box Adversarial Attack and Action Recognition are accepted by ICMR 2022! Congratulations to Siyuan and Yiran! <FONT COLOR="FF0000">[NEW]</FONT> <br /></p>
</li>
</ul>
<ul>
<li><p><i>2022/03/31</i> &ndash; 2 papers about Zero-shot Sketch-based Image Retrieval and Universal Cross-domain Retrieval are accepted by SIGIR 2022! Congratulations to Jialin and Kai! <FONT COLOR="FF0000">[NEW]</FONT> <br /></p>
</li>
</ul>
<ul>
<li><p><i>2022/03/09</i> &ndash; 3 papers about Cross-modal Retrieval, Video Localization and Human Pose Estimation are accepted by ICME 2022! Congratulations to Kai, Xun and Yixuan! <FONT COLOR="FF0000">[NEW]</FONT> <br /></p>
</li>
</ul>
<ul>
<li><p><i>2022/03/03</i> &ndash; 1 paper about Video Paragraph Grounding is accepted by CVPR 2022! Congratulations to Xun! <FONT COLOR="FF0000">[NEW]</FONT> <br /></p>
</li>
</ul>
<ul>
<li><p><i>2021/12/01</i> &ndash; 1 paper about Zero-shot Sketch-based Image Retrieval is accepted by AAAI 2022. Congratulations to Jialin! <FONT COLOR="FF0000">[NEW]</FONT> <br /></p>
</li>
</ul>
<h2>Highlighted Research</h2>
<table class="imgtable"><tr><td>
<img src="pictures/tmm2022_wang.jpg" alt="profile picture" width="116px" height="87px" />&nbsp;</td>
<td align="left"><p><b>Cross-Modal Dynamic Networks for Video Moment Retrieval With Text Query</b><br />
Gongmian Wang, <b>Xing Xu</b>, Fumin Shen, Huimin Lu, Yanli Ji, Heng Tao Shen.<br />
<i>IEEE Transactions on Multimedia</i>, 2022. <br />
[ <a href="https://ieeexplore.ieee.org/document/9681153">link</a> ]
[ <a href="https://github.com/CFM-MSG/Code_CDN">code</a> ]</p>
</td></tr></table>
<table class="imgtable"><tr><td>
<img src="pictures/tpami2021_wei.jpg" alt="profile picture" width="116px" height="87px" />&nbsp;</td>
<td align="left"><p><b>Universal Weighting Metric Learning for Cross-Modal Retrieval</b><br />
Jiwei Wei, Yang Yang, <b>Xing Xu</b>, Xiaofeng Zhu, Heng Tao Shen.<br />
<i>IEEE Transactions on Pattern Analysis and Machine Intelligence</i>, 2021. <br />
[ <a href="https://ieeexplore.ieee.org/document/9454290">link</a> ]
[ <a href="https://github.com/CFM-MSG/PolyLoss">code</a> ]</p>
</td></tr></table>
<table class="imgtable"><tr><td>
<img src="pictures/tpami2020_xing.jpg" alt="profile picture" width="116px" height="87px" />&nbsp;</td>
<td align="left"><p><b>Joint Feature Synthesis and Embedding: Adversarial Cross-modal Retrieval Revisited</b><br />
<b>Xing Xu</b>, Kaiyi Lin, Yang Yang, Alan Hanjalic, Heng Tao Shen.<br />
<i>IEEE Transactions on Pattern Analysis and Machine Intelligence</i>, 2020. <br />
[ <a href="https://ieeexplore.ieee.org/document/9296975/">link</a> ]
[ <a href="https://github.com/CFM-MSG/Code_JFSE">code</a> ]</p>
</td></tr></table>
<table class="imgtable"><tr><td>
<img src="pictures/tnnls2020_vqg.jpg" alt="profile picture" width="116px" height="87px" />&nbsp;</td>
<td align="left"><p><b>Radial Graph Convolutional Network for Visual Question Generation</b><br />
<b>Xing Xu</b>, Tan Wang, Yang Yang, Alan Hanjalic and Heng Tao Shen.<br />
<i>IEEE Transactions on Neural Networks and Learning Systems</i>, 2020. <br />
[ <a href="https://ieeexplore.ieee.org/document/9079208">link</a> ] 
[ <a href="https://github.com/Wangt-CN/VQG-GCN">code</a> ]</p>
</td></tr></table>
<table class="imgtable"><tr><td>
<img src="pictures/tnnls2020_xing.jpg" alt="profile picture" width="116px" height="87px" />&nbsp;</td>
<td align="left"><p><b>Cross-Modal Attention with Semantic Consistence for Image-Text Matching</b><br />
<b>Xing Xu</b>, Tan Wang, Yang Yang, Lin Zuo, Fumin Shen and Heng Tao Shen.<br />
<i>IEEE Transactions on Neural Networks and Learning Systems</i>, 2020. <br />
[ <a href="https://dl.acm.org/citation.cfm?id=3123326">link</a> ]
[ <a href="https://github.com/Wangt-CN/Code_CASC">code</a> ]</p>
</td></tr></table>
<table class="imgtable"><tr><td>
<img src="pictures/mm2019_tan.png" alt="profile picture" width="116px" height="87px" />&nbsp;</td>
<td align="left"><p><b>Matching Images and Text with Multi-modal Tensor Fusion and Re-ranking</b><br />
Tan Wang, <b>Xing Xu</b>, Yang Yang, Alan Hanjalic, Heng Tao Shen and Jingkuan Song.<br />
<i>ACM International Conference on Multimedia</i>, 2019. (*Corresponding Author) <br />
[ <a href="https://arxiv.org/abs/1908.04011">link</a> ] 
[ <a href="https://github.com/Wangt-CN/MTFN-RR-PyTorch-Code">code</a> ]</p>
</td></tr></table>
<table class="imgtable"><tr><td>
<img src="pictures/mm2017_bokun.jpg" alt="profile picture" width="116px" height="87px" />&nbsp;</td>
<td align="left"><p><b>Adversarial Cross-Modal Retrieval</b><br />
Bokun Wang, Yang Yang, <b>Xing Xu</b>, Alan Hanjalic and Heng Tao Shen.<br />
<i>ACM International Conference on Multimedia</i>, 2017. <FONT COLOR="FF0000">Best Paper Award</FONT><br />
[ <a href="https://dl.acm.org/citation.cfm?id=3123326">link</a> ] 
[ <a href="https://github.com/sunpeng981712364/ACMR_demo">code</a> ]</p>
</td></tr></table>
<table class="imgtable"><tr><td>
<img src="pictures/tip2017_xing.jpg" alt="profile picture" width="116px" height="87px" />&nbsp;</td>
<td align="left"><p><b>Learning Discriminative Binary Codes for Large-scale Cross-modal Retrieval</b><br />
<b>Xing Xu</b>, Fumin Shen, Yang Yang, Heng Tao Shen, Xuelong Li. <br />
<i>IEEE Transactions on Image Processing (TIP)</i>, 26:5, 2494-2507, 2017. <FONT COLOR="FF0000">ESI Highly Cited Paper</FONT><br />
[ <a href="https://ieeexplore.ieee.org/abstract/document/7867785/">link</a> ] 
[ <a href="http://cfm.uestc.edu.cn/~fshen/MFMR.zip">code</a> ]</p>
</td></tr></table>
<h2>Awards</h2>
<ul>
<li><p>å­¦æœ¯æ–°äººå¥–, ç”µå­ç§‘æŠ€å¤§å­¦, 2020</p>
</li>
<li><p>Best Paper Award, the 5th International Symposium on Artificial Intelligence and Robotics (ISAIR2020) <br /></p>
</li>
<li><p>IEEE Multimedia Prize Paper Award (2020) <br /></p>
</li>
<li><p>Outstanding Young Researcher Award, IEEE Computer Society, Special Technical Committee on Big Data, 2019 <br /></p>
</li>
<li><p>Best Paper Award, the 25th ACM International Conference on Multimedia (MM 2017) <br /></p>
</li>
<li><p>Worldâ€™s FIRST 10K Best Paper Award-Platinum Award, the 18th IEEE International Conference on Multimedia &amp; Expo (ICME 2017) <br /></p>
</li>
<li><p>Best Student Paper Award, the 28th Edition of the Australasian Database Conference (ADC 2017) <br /></p>
</li>
<li><p>Excellent Poster Award, the 21th Japan-Korea Joint Workshop on Frontiers of Computer Vision (FCV 2015) <br /></p>
</li>
</ul>
<h2>Teaching</h2>
<ul>
<li><p>Advanced Computer Vision (Graduate) <br /></p>
</li>
<li><p>Computer Vision and Pattern Recognition (Undergraduate) <br /></p>
</li>
<li><p>Principle and Application of Microcomputer (Foreign Undergraduate) <br /></p>
</li>
<li><p>Principle and Application of Microcomputer (Undergraduate) <br /></p>
</li>
</ul>
<div id="footer">
<div id="footer-text">
Page generated 2022-07-03 19:01:14 ÖĞ¹ú±ê×¼Ê±¼ä, by <a href="http://jemdoc.jaboc.net/">jemdoc</a>.
</div>
</div>
</td>
</tr>
</table>
</body>
</html>
